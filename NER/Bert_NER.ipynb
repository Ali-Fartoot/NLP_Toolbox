{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19c1887d",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "e5fccdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "train = load_from_disk(\"train.hf\")\n",
    "test = load_from_disk(\"test.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "dee024e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at E:\\ML\\NLP_Toolbox\\NER\\train.hf\\cache-aab69560c24eae2b.arrow\n"
     ]
    }
   ],
   "source": [
    "train = train.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "7d3c53f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at E:\\ML\\NLP_Toolbox\\NER\\test.hf\\cache-6c7c52e5435094d8.arrow\n"
     ]
    }
   ],
   "source": [
    "test = test.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85c4a9f",
   "metadata": {},
   "source": [
    "# Processing Bert without cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "777087e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "bert_model_name = \"HooshvareLab/bert-fa-base-uncased\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "2bdecf95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'جمعیت',\n",
       " 'جمعیت',\n",
       " 'این',\n",
       " 'دهستان',\n",
       " 'بر',\n",
       " 'اساس',\n",
       " 'سرشماری',\n",
       " 'سال',\n",
       " '۱۳۸۵',\n",
       " '(',\n",
       " '۱',\n",
       " '٬',\n",
       " '۲۱۲',\n",
       " '##خانوار',\n",
       " ')',\n",
       " '۵',\n",
       " '٬',\n",
       " '۱۱۸',\n",
       " '##نفر',\n",
       " 'بوده',\n",
       " 'است',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \" \".join(train[\"words\"][0])\n",
    "bert_tokens = bert_tokenizer(text).tokens()\n",
    "bert_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "dd43da29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'جمعیت جمعیت این دهستان بر اساس سرشماری سال ۱۳۸۵ ( ۱٬۲۱۲خانوار ) ۵٬۱۱۸نفر بوده است .'"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(train[\"words\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ec20e4",
   "metadata": {},
   "source": [
    "## Create a custom model for token classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "b0234ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertConfig\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "bert_model = BertModel.from_pretrained(\"HooshvareLab/bert-fa-base-uncased\",cache_dir=Path.cwd())\n",
    "bert_config = BertConfig.from_pretrained(\"HooshvareLab/bert-fa-base-uncased\",cache_dir=Path.cwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "acd1a74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers.models.bert.modeling_bert import BertPreTrainedModel\n",
    "from transformers.models.bert.modeling_bert import BertModel\n",
    "from torch import nn\n",
    "class ParsBertForTokenClassification(BertPreTrainedModel):\n",
    "    config_class = bert_config\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        # Load model body\n",
    "        self.parsbert = BertModel(config)\n",
    "        # Set up token classification head\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        # Load and initialize weights\n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, **kwargs):\n",
    "        \n",
    "        # Use model body to get encoder representations\n",
    "        outputs = self.parsbert(input_ids, attention_mask=attention_mask,\n",
    "        token_type_ids=token_type_ids, **kwargs)\n",
    "        \n",
    "        # Apply classifier to encoder representation\n",
    "        sequence_output = self.dropout(outputs[0])\n",
    "        logits = self.classifier(sequence_output)\n",
    "        \n",
    "        # Calculate losses\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            \n",
    "        # Return model output object\n",
    "        return TokenClassifierOutput(loss=loss, logits=logits,\n",
    "        hidden_states=outputs.hidden_states,\n",
    "        attentions=outputs.attentions)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "396c3949",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tags = train[\"ner\"]\n",
    "ner_tag_names = set(tag for tags in ner_tags for tag in tags)\n",
    "                \n",
    "index2tag = {idx: tag for idx, tag in enumerate(ner_tag_names)}\n",
    "tag2index = {tag: idx for idx, tag in enumerate(ner_tag_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "a7b8621a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "bert_config = AutoConfig.from_pretrained(bert_model_name,num_labels=4,\n",
    "                                            id2label=index2tag, label2id=tag2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "dde504e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ParsBertForTokenClassification were not initialized from the model checkpoint at HooshvareLab/bert-fa-base-uncased and are newly initialized: ['bert.parsbert.encoder.layer.0.attention.self.query.weight', 'bert.parsbert.encoder.layer.10.attention.self.value.bias', 'bert.parsbert.encoder.layer.10.output.LayerNorm.weight', 'bert.parsbert.pooler.dense.weight', 'bert.parsbert.encoder.layer.9.attention.self.value.weight', 'bert.parsbert.encoder.layer.5.attention.self.key.bias', 'bert.parsbert.encoder.layer.10.intermediate.dense.bias', 'bert.parsbert.encoder.layer.3.attention.output.dense.bias', 'bert.parsbert.embeddings.position_embeddings.weight', 'bert.parsbert.encoder.layer.11.output.LayerNorm.bias', 'bert.parsbert.encoder.layer.0.output.dense.weight', 'bert.parsbert.encoder.layer.1.intermediate.dense.bias', 'bert.parsbert.encoder.layer.5.attention.self.value.weight', 'bert.parsbert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.parsbert.encoder.layer.7.attention.self.value.weight', 'bert.parsbert.embeddings.word_embeddings.weight', 'bert.parsbert.encoder.layer.1.attention.self.query.weight', 'bert.parsbert.encoder.layer.7.attention.self.key.weight', 'bert.parsbert.encoder.layer.3.attention.self.value.weight', 'bert.parsbert.encoder.layer.10.attention.self.query.weight', 'bert.parsbert.encoder.layer.9.attention.self.value.bias', 'bert.parsbert.encoder.layer.0.attention.output.dense.weight', 'bert.parsbert.encoder.layer.10.attention.output.dense.weight', 'bert.parsbert.encoder.layer.8.attention.self.value.bias', 'bert.parsbert.encoder.layer.6.attention.self.value.bias', 'bert.parsbert.encoder.layer.9.attention.self.query.bias', 'bert.parsbert.encoder.layer.8.attention.self.key.bias', 'bert.parsbert.encoder.layer.1.attention.output.dense.bias', 'bert.parsbert.encoder.layer.6.output.LayerNorm.weight', 'bert.parsbert.encoder.layer.2.output.LayerNorm.bias', 'bert.parsbert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.parsbert.encoder.layer.7.attention.output.dense.weight', 'bert.parsbert.encoder.layer.1.attention.self.value.bias', 'bert.parsbert.encoder.layer.8.attention.self.query.bias', 'bert.parsbert.encoder.layer.4.attention.self.key.weight', 'bert.parsbert.encoder.layer.10.attention.self.query.bias', 'bert.parsbert.encoder.layer.7.attention.self.query.weight', 'bert.parsbert.encoder.layer.11.attention.self.key.weight', 'bert.parsbert.encoder.layer.0.attention.self.query.bias', 'bert.parsbert.encoder.layer.9.attention.output.dense.bias', 'bert.parsbert.encoder.layer.6.output.dense.weight', 'bert.parsbert.encoder.layer.6.attention.self.value.weight', 'bert.parsbert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.parsbert.encoder.layer.9.output.LayerNorm.bias', 'bert.parsbert.encoder.layer.1.output.LayerNorm.weight', 'bert.parsbert.encoder.layer.6.output.LayerNorm.bias', 'bert.parsbert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.parsbert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.parsbert.encoder.layer.2.output.dense.bias', 'bert.parsbert.encoder.layer.1.output.dense.weight', 'bert.parsbert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.parsbert.encoder.layer.9.attention.self.query.weight', 'bert.parsbert.encoder.layer.0.intermediate.dense.weight', 'bert.parsbert.encoder.layer.0.attention.self.value.bias', 'bert.parsbert.encoder.layer.0.attention.self.key.bias', 'bert.parsbert.encoder.layer.3.attention.self.key.bias', 'bert.parsbert.pooler.dense.bias', 'bert.parsbert.encoder.layer.0.intermediate.dense.bias', 'bert.parsbert.encoder.layer.1.attention.self.query.bias', 'bert.parsbert.encoder.layer.6.attention.self.key.bias', 'bert.parsbert.encoder.layer.6.attention.self.key.weight', 'bert.parsbert.encoder.layer.2.attention.self.key.bias', 'bert.parsbert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.parsbert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.parsbert.encoder.layer.11.intermediate.dense.weight', 'bert.parsbert.encoder.layer.3.attention.output.dense.weight', 'bert.parsbert.encoder.layer.2.attention.self.query.bias', 'bert.parsbert.encoder.layer.2.attention.self.key.weight', 'bert.parsbert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.parsbert.encoder.layer.2.output.dense.weight', 'bert.parsbert.encoder.layer.1.output.dense.bias', 'bert.parsbert.encoder.layer.4.attention.self.key.bias', 'bert.parsbert.encoder.layer.8.attention.output.dense.weight', 'bert.parsbert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.parsbert.encoder.layer.8.attention.self.query.weight', 'bert.parsbert.encoder.layer.4.intermediate.dense.bias', 'bert.parsbert.encoder.layer.4.output.LayerNorm.bias', 'bert.parsbert.encoder.layer.9.intermediate.dense.bias', 'bert.parsbert.encoder.layer.0.output.dense.bias', 'bert.parsbert.encoder.layer.9.attention.self.key.weight', 'bert.parsbert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.parsbert.encoder.layer.7.intermediate.dense.weight', 'bert.parsbert.encoder.layer.2.intermediate.dense.bias', 'bert.parsbert.encoder.layer.9.output.dense.bias', 'bert.parsbert.encoder.layer.4.attention.output.dense.weight', 'bert.parsbert.encoder.layer.3.intermediate.dense.weight', 'bert.parsbert.encoder.layer.8.intermediate.dense.weight', 'bert.parsbert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.parsbert.encoder.layer.3.output.dense.weight', 'bert.parsbert.encoder.layer.3.output.LayerNorm.weight', 'bert.parsbert.encoder.layer.0.attention.self.key.weight', 'bert.parsbert.encoder.layer.2.attention.output.dense.bias', 'bert.parsbert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.parsbert.encoder.layer.9.attention.output.dense.weight', 'bert.parsbert.encoder.layer.7.attention.output.dense.bias', 'bert.parsbert.encoder.layer.10.output.dense.weight', 'bert.parsbert.encoder.layer.8.attention.self.key.weight', 'bert.parsbert.encoder.layer.10.output.LayerNorm.bias', 'bert.classifier.weight', 'bert.parsbert.encoder.layer.7.output.LayerNorm.weight', 'bert.parsbert.encoder.layer.5.attention.self.key.weight', 'bert.parsbert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.parsbert.encoder.layer.6.attention.output.dense.bias', 'bert.parsbert.encoder.layer.9.intermediate.dense.weight', 'bert.parsbert.encoder.layer.7.output.dense.bias', 'bert.parsbert.encoder.layer.10.attention.self.key.bias', 'bert.parsbert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.parsbert.encoder.layer.7.output.LayerNorm.bias', 'bert.parsbert.encoder.layer.2.intermediate.dense.weight', 'bert.parsbert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.parsbert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.parsbert.embeddings.LayerNorm.weight', 'bert.parsbert.encoder.layer.2.attention.self.query.weight', 'bert.parsbert.encoder.layer.3.attention.self.value.bias', 'bert.parsbert.encoder.layer.0.output.LayerNorm.weight', 'bert.parsbert.encoder.layer.3.output.LayerNorm.bias', 'bert.parsbert.encoder.layer.1.attention.output.dense.weight', 'bert.parsbert.encoder.layer.8.output.LayerNorm.weight', 'bert.parsbert.embeddings.LayerNorm.bias', 'bert.parsbert.encoder.layer.2.attention.self.value.bias', 'bert.parsbert.encoder.layer.8.output.dense.weight', 'bert.parsbert.encoder.layer.6.attention.output.dense.weight', 'bert.parsbert.encoder.layer.5.attention.self.query.weight', 'bert.parsbert.encoder.layer.4.attention.output.dense.bias', 'bert.parsbert.encoder.layer.5.attention.self.value.bias', 'bert.parsbert.encoder.layer.11.attention.self.value.bias', 'bert.parsbert.encoder.layer.5.intermediate.dense.bias', 'bert.parsbert.encoder.layer.5.output.dense.weight', 'bert.parsbert.encoder.layer.8.attention.self.value.weight', 'bert.parsbert.encoder.layer.4.attention.self.query.bias', 'bert.parsbert.encoder.layer.11.attention.self.query.weight', 'bert.parsbert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.parsbert.encoder.layer.11.output.dense.bias', 'bert.parsbert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.parsbert.encoder.layer.9.output.dense.weight', 'bert.parsbert.encoder.layer.11.attention.output.dense.bias', 'bert.parsbert.encoder.layer.6.output.dense.bias', 'bert.parsbert.encoder.layer.8.output.dense.bias', 'bert.parsbert.encoder.layer.7.output.dense.weight', 'bert.parsbert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.parsbert.encoder.layer.7.attention.self.value.bias', 'bert.parsbert.encoder.layer.0.attention.output.dense.bias', 'bert.parsbert.encoder.layer.3.attention.self.key.weight', 'bert.parsbert.encoder.layer.5.output.LayerNorm.weight', 'bert.parsbert.encoder.layer.10.attention.self.value.weight', 'bert.parsbert.encoder.layer.5.output.dense.bias', 'bert.parsbert.encoder.layer.6.attention.self.query.bias', 'bert.parsbert.encoder.layer.5.attention.output.dense.weight', 'bert.parsbert.encoder.layer.6.intermediate.dense.weight', 'bert.parsbert.encoder.layer.11.output.LayerNorm.weight', 'bert.parsbert.embeddings.token_type_embeddings.weight', 'bert.parsbert.encoder.layer.11.attention.self.key.bias', 'bert.parsbert.encoder.layer.8.output.LayerNorm.bias', 'bert.parsbert.encoder.layer.1.output.LayerNorm.bias', 'bert.parsbert.encoder.layer.0.attention.self.value.weight', 'bert.parsbert.encoder.layer.11.attention.output.dense.weight', 'bert.parsbert.encoder.layer.1.intermediate.dense.weight', 'bert.parsbert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.parsbert.encoder.layer.4.attention.self.query.weight', 'bert.parsbert.encoder.layer.11.attention.self.value.weight', 'bert.parsbert.encoder.layer.1.attention.self.key.bias', 'bert.parsbert.encoder.layer.5.output.LayerNorm.bias', 'bert.parsbert.encoder.layer.5.attention.output.dense.bias', 'bert.parsbert.encoder.layer.0.output.LayerNorm.bias', 'bert.parsbert.encoder.layer.4.output.dense.bias', 'bert.parsbert.encoder.layer.6.attention.self.query.weight', 'bert.parsbert.encoder.layer.5.attention.self.query.bias', 'bert.parsbert.encoder.layer.10.output.dense.bias', 'bert.parsbert.encoder.layer.10.attention.output.dense.bias', 'bert.parsbert.encoder.layer.4.attention.self.value.weight', 'bert.parsbert.encoder.layer.9.output.LayerNorm.weight', 'bert.parsbert.encoder.layer.1.attention.self.value.weight', 'bert.parsbert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.parsbert.encoder.layer.5.intermediate.dense.weight', 'bert.parsbert.encoder.layer.9.attention.self.key.bias', 'bert.parsbert.encoder.layer.7.attention.self.key.bias', 'bert.parsbert.encoder.layer.3.intermediate.dense.bias', 'bert.parsbert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.parsbert.encoder.layer.2.output.LayerNorm.weight', 'bert.parsbert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.parsbert.encoder.layer.4.output.LayerNorm.weight', 'bert.parsbert.encoder.layer.8.intermediate.dense.bias', 'bert.parsbert.encoder.layer.3.output.dense.bias', 'bert.parsbert.encoder.layer.1.attention.self.key.weight', 'bert.parsbert.encoder.layer.7.intermediate.dense.bias', 'bert.parsbert.encoder.layer.2.attention.self.value.weight', 'bert.parsbert.encoder.layer.8.attention.output.dense.bias', 'bert.parsbert.encoder.layer.10.attention.self.key.weight', 'bert.classifier.bias', 'bert.parsbert.encoder.layer.4.intermediate.dense.weight', 'bert.parsbert.encoder.layer.4.output.dense.weight', 'bert.parsbert.encoder.layer.7.attention.self.query.bias', 'bert.parsbert.encoder.layer.3.attention.self.query.bias', 'bert.parsbert.encoder.layer.3.attention.self.query.weight', 'bert.parsbert.encoder.layer.2.attention.output.dense.weight', 'bert.parsbert.encoder.layer.4.attention.self.value.bias', 'bert.parsbert.encoder.layer.11.output.dense.weight', 'bert.parsbert.encoder.layer.11.attention.self.query.bias', 'bert.parsbert.encoder.layer.10.intermediate.dense.weight', 'bert.parsbert.encoder.layer.11.intermediate.dense.bias', 'bert.parsbert.encoder.layer.6.intermediate.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "parsbert_model = (ParsBertForTokenClassification\n",
    "                  .from_pretrained(bert_model_name, config=bert_config,cache_dir=Path.cwd())\n",
    "                  .to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "965295c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>jack</td>\n",
       "      <td>spar</td>\n",
       "      <td>##row</td>\n",
       "      <td>love</td>\n",
       "      <td>##s</td>\n",
       "      <td>new</td>\n",
       "      <td>york</td>\n",
       "      <td>!</td>\n",
       "      <td>[SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Input IDs</th>\n",
       "      <td>2</td>\n",
       "      <td>37223</td>\n",
       "      <td>77258</td>\n",
       "      <td>36783</td>\n",
       "      <td>27594</td>\n",
       "      <td>2032</td>\n",
       "      <td>14745</td>\n",
       "      <td>45148</td>\n",
       "      <td>1001</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0      1      2      3      4     5      6      7     8      9\n",
       "Tokens     [CLS]   jack   spar  ##row   love   ##s    new   york     !  [SEP]\n",
       "Input IDs      2  37223  77258  36783  27594  2032  14745  45148  1001      4"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "en_text = \"Jack Sparrow loves New York!\"\n",
    "input_ids = bert_tokenizer.encode(en_text, return_tensors=\"pt\")\n",
    "bert_tokens = bert_tokenizer(en_text).tokens()\n",
    "pd.DataFrame([bert_tokens, input_ids[0].numpy()], index=[\"Tokens\", \"Input IDs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "df3da471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>جمعیت</td>\n",
       "      <td>جمعیت</td>\n",
       "      <td>این</td>\n",
       "      <td>دهستان</td>\n",
       "      <td>بر</td>\n",
       "      <td>اساس</td>\n",
       "      <td>سرشماری</td>\n",
       "      <td>سال</td>\n",
       "      <td>۱۳۸۵</td>\n",
       "      <td>(</td>\n",
       "      <td>۱</td>\n",
       "      <td>٬</td>\n",
       "      <td>۲۱۲</td>\n",
       "      <td>##خانوار</td>\n",
       "      <td>)</td>\n",
       "      <td>۵</td>\n",
       "      <td>٬</td>\n",
       "      <td>۱۱۸</td>\n",
       "      <td>##نفر</td>\n",
       "      <td>بوده</td>\n",
       "      <td>است</td>\n",
       "      <td>.</td>\n",
       "      <td>[SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Input IDs</th>\n",
       "      <td>2</td>\n",
       "      <td>4172</td>\n",
       "      <td>4172</td>\n",
       "      <td>2802</td>\n",
       "      <td>8806</td>\n",
       "      <td>2801</td>\n",
       "      <td>3561</td>\n",
       "      <td>8907</td>\n",
       "      <td>2844</td>\n",
       "      <td>9815</td>\n",
       "      <td>1006</td>\n",
       "      <td>1455</td>\n",
       "      <td>1394</td>\n",
       "      <td>30064</td>\n",
       "      <td>92751</td>\n",
       "      <td>1007</td>\n",
       "      <td>1459</td>\n",
       "      <td>1394</td>\n",
       "      <td>15658</td>\n",
       "      <td>18783</td>\n",
       "      <td>3225</td>\n",
       "      <td>2806</td>\n",
       "      <td>1012</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0      1      2     3       4     5     6        7     8     9   \\\n",
       "Tokens     [CLS]  جمعیت  جمعیت   این  دهستان    بر  اساس  سرشماری   سال  ۱۳۸۵   \n",
       "Input IDs      2   4172   4172  2802    8806  2801  3561     8907  2844  9815   \n",
       "\n",
       "             10    11    12     13        14    15    16    17     18     19  \\\n",
       "Tokens        (     ۱     ٬    ۲۱۲  ##خانوار     )     ۵     ٬    ۱۱۸  ##نفر   \n",
       "Input IDs  1006  1455  1394  30064     92751  1007  1459  1394  15658  18783   \n",
       "\n",
       "             20    21    22     23  \n",
       "Tokens     بوده   است     .  [SEP]  \n",
       "Input IDs  3225  2806  1012      4  "
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "per_text = \" \".join(train['words'][0])\n",
    "input_ids = bert_tokenizer.encode(per_text, return_tensors=\"pt\")\n",
    "bert_tokens = bert_tokenizer(per_text).tokens()\n",
    "pd.DataFrame([bert_tokens, input_ids[0].numpy()], index=[\"Tokens\", \"Input IDs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "d7ea4868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in sequence: 24\n",
      "Shape of outputs: torch.Size([1, 24, 4])\n"
     ]
    }
   ],
   "source": [
    "outputs = parsbert_model(input_ids.to(device)).logits\n",
    "predictions = torch.argmax(outputs, dim=-1)\n",
    "print(f\"Number of tokens in sequence: {len(bert_tokens)}\")\n",
    "print(f\"Shape of outputs: {outputs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "df764a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_text(text, tags, model, tokenizer):\n",
    "    tags = list(ner_tag_names)\n",
    "    \n",
    "    # Get tokens with special characters\n",
    "    tokens = bert_tokenizer(text).tokens()\n",
    "    # Encode the sequence into IDs\n",
    "    input_ids = bert_tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    # Get predictions as distribution over 7 possible classes\n",
    "    outputs = model(inputs)[0]\n",
    "    # Take argmax to get most likely class per token\n",
    "    predictions = torch.argmax(outputs, dim=2)\n",
    "    # Convert to DataFrame\n",
    "    preds = [tags[p] for p in predictions[0].cpu().numpy()]\n",
    "    return pd.DataFrame([tokens, preds], index=[\"Tokens\", \"Tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "8815a016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = bert_tokenizer(examples[\"words\"], truncation=True,\n",
    "    is_split_into_words=True)\n",
    "    labels = []\n",
    "    for idx, label in enumerate(examples[\"ner\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None or word_idx == previous_word_idx:\n",
    "                label_ids.append('-100')\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "d77ddf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_panx_dataset(corpus):\n",
    "    return corpus.map(tokenize_and_align_labels,batched=True,\n",
    "                      remove_columns=['id','words','lang','ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "e4bd2508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/733507 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_encoded = encode_panx_dataset(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadb8d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_encoded['words'][0])\n",
    "print('----------------------')\n",
    "print(train_encoded['labels'][0])\n",
    "print('----------------------')\n",
    "print(train['ner'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96e37ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "for i in range(1000):\n",
    "    if len(train_encoded['labels'][i]) ==  len(train_encoded['input_ids'][i]):\n",
    "        errors.append(i)\n",
    "\n",
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de0d135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_predictions(predictions, label_ids):\n",
    "        preds = np.argmax(predictions, axis=2)\n",
    "        batch_size, seq_len = preds.shape\n",
    "        labels_list, preds_list = [], []\n",
    "        for batch_idx in range(batch_size):\n",
    "            example_labels, example_preds = [], []\n",
    "            for seq_idx in range(seq_len):\n",
    "                # Ignore label IDs = -100\n",
    "                if label_ids[batch_idx, seq_idx] != -100:\n",
    "                    example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])\n",
    "                    example_preds.append(index2tag[preds[batch_idx][seq_idx]])\n",
    "            labels_list.append(example_labels)\n",
    "            preds_list.append(example_preds)\n",
    "        return preds_list, labels_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Pytorch]",
   "language": "python",
   "name": "conda-env-Pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
