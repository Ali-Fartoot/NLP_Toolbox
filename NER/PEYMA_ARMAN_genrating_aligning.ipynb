{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "005633c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import pathlib\n",
    "import os\n",
    "from datasets import Dataset ,concatenate_datasets, DatasetDict , Sequence , Value , Features , ClassLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a99a8e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tags(labels):\n",
    "    '''\n",
    "    Convert each label to a unique label (aligning labels for two datasets)\n",
    "    '''\n",
    "    maps = {'B-loc':'B_LOC', 'I-loc':'I_LOC', 'B-pers':'B_PER','I-pers':'I_PER',\n",
    "              'B-org':'B_ORG', 'I-org':'I_ORG', 'B-fac':'B_FAC', 'I-fac':'I_FAC',\n",
    "              'B-event' : 'B_EVE', 'I-event' : 'I_EVE', 'B-pro':'B_PRO', 'I-pro':'I_PRO',\n",
    "              'I-fac': 'I_FAC', 'B-fac': 'B_FAC'\n",
    "             }\n",
    "    for index in range(len(labels)):\n",
    "        if labels[index] in maps.keys():\n",
    "            labels[index] =  maps[labels[index]]\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32918021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(token_path,label_path,shuffle=True):\n",
    "    '''\n",
    "    Genrate dataset -> reading from raw txt data and split \n",
    "                       labels and tokens. Apply convert_tags to align datasests\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # List of lists for tokens and labels\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    \n",
    "    with open(token_path , encoding='utf-8' , mode='r') as t , open(label_path , encoding='utf-8' , mode='r') as l:\n",
    "        \n",
    "        for row in t:\n",
    "            tokens.append(row.strip('\\n').split())\n",
    "        \n",
    "        for row in l :\n",
    "            labels.append(row.strip('\\n').split())\n",
    "\n",
    "    # Verifying data\n",
    "    if len(tokens) != len(labels):\n",
    "        raise Exception(\"Number of rows in tokens and labels doesn't match!\")\n",
    "    # aligning\n",
    "    labels = list(map(convert_tags,labels))\n",
    "    \n",
    "    # Verifying labels after aligning\n",
    "    for token , label in zip(tokens,labels):\n",
    "        if len(token) != len(label):\n",
    "            raise Exception(\"Number of tokens and labels in the first row doesn't match!\")\n",
    "    \n",
    "\n",
    "\n",
    "    # Finding unique tags in labels\n",
    "    tags =  list(set(['B_DAT', 'B_LOC', 'B_MON', 'B_ORG', 'B_PCT', 'B_PER', 'B_TIM', 'I_DAT',\n",
    "            'I_LOC', 'I_MON', 'I_ORG', 'I_PCT', 'I_PER', 'I_TIM',\n",
    "            'B_LOC', 'I_LOC', 'B_PER', 'I_PER', 'B_ORG', 'I_ORG', 'B_FAC', 'I_FAC',\n",
    "            'B_EVE', 'I_EVE', 'B_PRO', 'I_PRO','O']))\n",
    "    tags.sort()\n",
    "    \n",
    "    # Verifying tags\n",
    "    if '\\n' in tags or '' in tags:\n",
    "        raise Exception('Wrong tags detected!')\n",
    "    \n",
    "    # Creating Dataset object\n",
    "    dataset_dict = {\n",
    "        'tokens' : tokens,\n",
    "        'ner_tags' : labels\n",
    "    }\n",
    "    \n",
    "    ner_tags = ClassLabel(num_classes=len(tags) , names=tags)\n",
    "    dataset = Dataset.from_dict(\n",
    "        mapping = dataset_dict,\n",
    "        features = Features({\n",
    "            'tokens' : Sequence(feature=Value('string')),\n",
    "            'ner_tags' : Sequence(ner_tags)\n",
    "        })\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Returning train, test and validation splits\n",
    "    train_split = round(dataset.num_rows * 0.8)\n",
    "    test_split = round(dataset.num_rows * 0.9)\n",
    "    \n",
    "    train = dataset.select(range(0,train_split))\n",
    "    test = dataset.select(range(train_split,test_split))\n",
    "    validation = dataset.select(range(test_split,dataset.num_rows))\n",
    "    \n",
    "    return train , test , validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c90c777a",
   "metadata": {},
   "outputs": [],
   "source": [
    "arman_tokens = './PEYMA_ARMAN/arman-tokens.txt'\n",
    "arman_labels = './PEYMA_ARMAN/arman-labels.txt'\n",
    "\n",
    "peyma_tokens = './PEYMA_ARMAN/peyma-tokens.txt'\n",
    "peyma_labels = './PEYMA_ARMAN/peyma-labels.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88edffcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating two datasets\n",
    "corpus = DatasetDict()\n",
    "corpus_arman = DatasetDict()\n",
    "corpus_peyma = DatasetDict()\n",
    "corpus_arman['train'] , corpus_arman['test'] , corpus_arman['validation'] = generate_dataset(arman_tokens,arman_labels)\n",
    "corpus_peyma['train'] , corpus_peyma['test'] , corpus_peyma['validation']  = generate_dataset(peyma_tokens,peyma_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f721158a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate two datsets two unique datset.\n",
    "corpus['train'] = concatenate_datasets([corpus_arman['train'],corpus_peyma['train']])\n",
    "corpus['test'] = concatenate_datasets([corpus_arman['test'],corpus_peyma['test']])\n",
    "corpus['validation'] = concatenate_datasets([corpus_arman['validation'],corpus_peyma['validation']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c89010c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_tag_names(batch):\n",
    "    \"\"\"\n",
    "   Convert each number label to its value for improve readability\n",
    "                                           and return new column\n",
    "\n",
    "    \"\"\"\n",
    "    return {'ner_tags_names' : [tags.int2str(idx) for idx in batch['ner_tags']]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56a52644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/26417 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3303 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3302 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply that function\n",
    "tags = corpus['train'].features['ner_tags'].feature\n",
    "corpus = corpus.map(create_tag_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a170be8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/26417 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3303 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3302 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Shuffle and save dataset\n",
    "corpus = corpus.shuffle(42)\n",
    "corpus.save_to_disk(\"PEYMA_ARMAN_MIXED.hf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Pytorch]",
   "language": "python",
   "name": "conda-env-Pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
