{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "005633c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import pathlib\n",
    "import os\n",
    "from datasets import Dataset ,load_dataset,concatenate_datasets, DatasetDict , Sequence , Value , Features , ClassLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a99a8e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tags(labels):\n",
    "    '''\n",
    "    Convert each label to a unique label (aligning labels for two datasets)\n",
    "    '''\n",
    "    maps = {'B-loc':'B_LOC', 'I-loc':'I_LOC', 'B-pers':'B_PER','I-pers':'I_PER',\n",
    "              'B-org':'B_ORG', 'I-org':'I_ORG', 'B-fac':'B_FAC', 'I-fac':'I_FAC',\n",
    "              'B-event' : 'B_EVE', 'I-event' : 'I_EVE', 'B-pro':'B_PRO', 'I-pro':'I_PRO',\n",
    "              'I-fac': 'I_FAC', 'B-fac': 'B_FAC'\n",
    "             }\n",
    "    for index in range(len(labels)):\n",
    "        if labels[index] in maps.keys():\n",
    "            labels[index] =  maps[labels[index]]\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32918021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(token_path,label_path,shuffle=True):\n",
    "    '''\n",
    "    Genrate dataset -> reading from raw txt data and split \n",
    "                       labels and tokens. Apply convert_tags to align datasests\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # List of lists for tokens and labels\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    \n",
    "    with open(token_path , encoding='utf-8' , mode='r') as t , open(label_path , encoding='utf-8' , mode='r') as l:\n",
    "        \n",
    "        for row in t:\n",
    "            tokens.append(row.strip('\\n').split())\n",
    "        \n",
    "        for row in l :\n",
    "            labels.append(row.strip('\\n').split())\n",
    "\n",
    "    # Verifying data\n",
    "    if len(tokens) != len(labels):\n",
    "        raise Exception(\"Number of rows in tokens and labels doesn't match!\")\n",
    "    # aligning\n",
    "    labels = list(map(convert_tags,labels))\n",
    "    \n",
    "    # Verifying labels after aligning\n",
    "    for token , label in zip(tokens,labels):\n",
    "        if len(token) != len(label):\n",
    "            raise Exception(\"Number of tokens and labels in the first row doesn't match!\")\n",
    "    \n",
    "\n",
    "\n",
    "    # Finding unique tags in labels\n",
    "    tags =  list(set(['B_DAT', 'B_LOC', 'B_MON', 'B_ORG', 'B_PCT', 'B_PER', 'B_TIM', 'I_DAT',\n",
    "            'I_LOC', 'I_MON', 'I_ORG', 'I_PCT', 'I_PER', 'I_TIM',\n",
    "            'B_LOC', 'I_LOC', 'B_PER', 'I_PER', 'B_ORG', 'I_ORG', 'B_FAC', 'I_FAC',\n",
    "            'B_EVE', 'I_EVE', 'B_PRO', 'I_PRO','O']))\n",
    "    tags.sort()\n",
    "    \n",
    "    # Verifying tags\n",
    "    if '\\n' in tags or '' in tags:\n",
    "        raise Exception('Wrong tags detected!')\n",
    "    \n",
    "    # Creating Dataset object\n",
    "    dataset_dict = {\n",
    "        'tokens' : tokens,\n",
    "        'ner_tags' : labels\n",
    "    }\n",
    "    \n",
    "    ner_tags = ClassLabel(num_classes=len(tags) , names=tags)\n",
    "    dataset = Dataset.from_dict(\n",
    "        mapping = dataset_dict,\n",
    "        features = Features({\n",
    "            'tokens' : Sequence(feature=Value('string')),\n",
    "            'ner_tags' : Sequence(ner_tags)\n",
    "        })\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Returning train, test and validation splits\n",
    "    train_split = round(dataset.num_rows * 0.8)\n",
    "    test_split = round(dataset.num_rows * 0.9)\n",
    "    \n",
    "    train = dataset.select(range(0,train_split))\n",
    "    test = dataset.select(range(train_split,test_split))\n",
    "    validation = dataset.select(range(test_split,dataset.num_rows))\n",
    "    \n",
    "    return train , test , validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c90c777a",
   "metadata": {},
   "outputs": [],
   "source": [
    "arman_tokens = './PEYMA_ARMAN/arman-tokens.txt'\n",
    "arman_labels = './PEYMA_ARMAN/arman-labels.txt'\n",
    "\n",
    "peyma_tokens = './PEYMA_ARMAN/peyma-tokens.txt'\n",
    "peyma_labels = './PEYMA_ARMAN/peyma-labels.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88edffcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating two datasets\n",
    "corpus = DatasetDict()\n",
    "corpus_arman = DatasetDict()\n",
    "corpus_peyma = DatasetDict()\n",
    "corpus_arman['train'] , corpus_arman['test'] , corpus_arman['validation'] = generate_dataset(arman_tokens,arman_labels)\n",
    "corpus_peyma['train'] , corpus_peyma['test'] , corpus_peyma['validation']  = generate_dataset(peyma_tokens,peyma_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f721158a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate two datsets two unique datset.\n",
    "corpus['train'] = concatenate_datasets([corpus_arman['train'],corpus_peyma['train']])\n",
    "corpus['test'] = concatenate_datasets([corpus_arman['test'],corpus_peyma['test']])\n",
    "corpus['validation'] = concatenate_datasets([corpus_arman['validation'],corpus_peyma['validation']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c89010c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_tag_names(batch):\n",
    "    \"\"\"\n",
    "   Convert each number label to its value for improve readability\n",
    "                                           and return new column\n",
    "\n",
    "    \"\"\"\n",
    "    return {'ner_tags_names' : [tags.int2str(idx) for idx in batch['ner_tags']]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56a52644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/26417 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3303 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3302 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply that function\n",
    "tags = corpus['train'].features['ner_tags'].feature\n",
    "corpus = corpus.map(create_tag_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a170be8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/26417 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3303 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3302 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Shuffle and save dataset\n",
    "corpus = corpus.shuffle(42)\n",
    "corpus.save_to_disk(\"PEYMA_ARMAN_Mixed.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec6b6d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = corpus['train'].to_pandas()\n",
    "df_test = corpus['test'].to_pandas()\n",
    "df_validation = corpus['validation'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2620dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('train.csv')\n",
    "df_test.to_csv('test.csv')\n",
    "df_validation.to_csv('validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6223700f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "{'B_DAT': 1512, 'B_EVE': 1379, 'B_FAC': 1334, 'B_LOC': 13040, 'B_MON': 446, 'B_ORG': 15762, 'B_PCT': 266, 'B_PER': 11371, 'B_PRO': 1719, 'B_TIM': 224, 'I_DAT': 1939, 'I_EVE': 4600, 'I_FAC': 2222, 'I_LOC': 4254, 'I_MON': 1314, 'I_ORG': 21347, 'I_PCT': 308, 'I_PER': 7160, 'I_PRO': 1736, 'I_TIM': 375, 'O': 747216}\n",
      "test\n",
      "{'B_DAT': 185, 'B_EVE': 218, 'B_FAC': 124, 'B_LOC': 1868, 'B_MON': 53, 'B_ORG': 2017, 'B_PCT': 27, 'B_PER': 1566, 'B_PRO': 281, 'B_TIM': 27, 'I_DAT': 245, 'I_EVE': 697, 'I_FAC': 237, 'I_LOC': 511, 'I_MON': 142, 'I_ORG': 2843, 'I_PCT': 31, 'I_PER': 1075, 'I_PRO': 345, 'I_TIM': 37, 'O': 92214}\n",
      "validation\n",
      "{'B_DAT': 161, 'B_EVE': 143, 'B_FAC': 192, 'B_LOC': 1539, 'B_MON': 28, 'B_ORG': 2180, 'B_PCT': 33, 'B_PER': 1335, 'B_PRO': 172, 'B_TIM': 30, 'I_DAT': 217, 'I_EVE': 520, 'I_FAC': 349, 'I_LOC': 494, 'I_MON': 54, 'I_ORG': 2923, 'I_PCT': 34, 'I_PER': 813, 'I_PRO': 136, 'I_TIM': 39, 'O': 96857}\n",
      "            B_DAT  B_EVE  B_FAC  B_LOC  B_MON  B_ORG  B_PCT  B_PER  B_PRO  \\\n",
      "train        1512   1379   1334  13040    446  15762    266  11371   1719   \n",
      "test          185    218    124   1868     53   2017     27   1566    281   \n",
      "validation    161    143    192   1539     28   2180     33   1335    172   \n",
      "\n",
      "            B_TIM  ...  I_EVE  I_FAC  I_LOC  I_MON  I_ORG  I_PCT  I_PER  \\\n",
      "train         224  ...   4600   2222   4254   1314  21347    308   7160   \n",
      "test           27  ...    697    237    511    142   2843     31   1075   \n",
      "validation     30  ...    520    349    494     54   2923     34    813   \n",
      "\n",
      "            I_PRO  I_TIM       O  \n",
      "train        1736    375  747216  \n",
      "test          345     37   92214  \n",
      "validation    136     39   96857  \n",
      "\n",
      "[3 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tag_counts = {\n",
    "    \"B_DAT\": 0, \"B_EVE\": 0, \"B_FAC\": 0, \"B_LOC\": 0, \"B_MON\": 0,\n",
    "    \"B_ORG\": 0, \"B_PCT\": 0, \"B_PER\": 0, \"B_PRO\": 0, \"B_TIM\": 0,\n",
    "    \"I_DAT\": 0, \"I_EVE\": 0, \"I_FAC\": 0, \"I_LOC\": 0, \"I_MON\": 0,\n",
    "    \"I_ORG\": 0, \"I_PCT\": 0, \"I_PER\": 0, \"I_PRO\": 0, \"I_TIM\": 0,\n",
    "    \"O\":0\n",
    "}\n",
    "dicts = {}\n",
    "# Iterate through each split of the dataset\n",
    "splits = [\"train\",\"test\", \"validation\" ]\n",
    "for split in splits:\n",
    "    print(split)\n",
    "    ner_tags = corpus[split][\"ner_tags_names\"]\n",
    "    tag_counts_temp = tag_counts.copy()\n",
    "    for tags in ner_tags:\n",
    "        for tag in tags:\n",
    "            tag_counts_temp[tag] += 1\n",
    "    print(tag_counts_temp)\n",
    "    dicts[split] = tag_counts_temp\n",
    "    del(tag_counts_temp)\n",
    "\n",
    "# Convert tag_counts dictionary to DataFrame\n",
    "df = pd.DataFrame.from_dict(dicts, orient=\"index\", columns=tag_counts.keys())\n",
    "\n",
    "\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b23a1cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B_DAT</th>\n",
       "      <th>B_EVE</th>\n",
       "      <th>B_FAC</th>\n",
       "      <th>B_LOC</th>\n",
       "      <th>B_MON</th>\n",
       "      <th>B_ORG</th>\n",
       "      <th>B_PCT</th>\n",
       "      <th>B_PER</th>\n",
       "      <th>B_PRO</th>\n",
       "      <th>B_TIM</th>\n",
       "      <th>...</th>\n",
       "      <th>I_EVE</th>\n",
       "      <th>I_FAC</th>\n",
       "      <th>I_LOC</th>\n",
       "      <th>I_MON</th>\n",
       "      <th>I_ORG</th>\n",
       "      <th>I_PCT</th>\n",
       "      <th>I_PER</th>\n",
       "      <th>I_PRO</th>\n",
       "      <th>I_TIM</th>\n",
       "      <th>O</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>1512</td>\n",
       "      <td>1379</td>\n",
       "      <td>1334</td>\n",
       "      <td>13040</td>\n",
       "      <td>446</td>\n",
       "      <td>15762</td>\n",
       "      <td>266</td>\n",
       "      <td>11371</td>\n",
       "      <td>1719</td>\n",
       "      <td>224</td>\n",
       "      <td>...</td>\n",
       "      <td>4600</td>\n",
       "      <td>2222</td>\n",
       "      <td>4254</td>\n",
       "      <td>1314</td>\n",
       "      <td>21347</td>\n",
       "      <td>308</td>\n",
       "      <td>7160</td>\n",
       "      <td>1736</td>\n",
       "      <td>375</td>\n",
       "      <td>747216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>185</td>\n",
       "      <td>218</td>\n",
       "      <td>124</td>\n",
       "      <td>1868</td>\n",
       "      <td>53</td>\n",
       "      <td>2017</td>\n",
       "      <td>27</td>\n",
       "      <td>1566</td>\n",
       "      <td>281</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>697</td>\n",
       "      <td>237</td>\n",
       "      <td>511</td>\n",
       "      <td>142</td>\n",
       "      <td>2843</td>\n",
       "      <td>31</td>\n",
       "      <td>1075</td>\n",
       "      <td>345</td>\n",
       "      <td>37</td>\n",
       "      <td>92214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation</th>\n",
       "      <td>161</td>\n",
       "      <td>143</td>\n",
       "      <td>192</td>\n",
       "      <td>1539</td>\n",
       "      <td>28</td>\n",
       "      <td>2180</td>\n",
       "      <td>33</td>\n",
       "      <td>1335</td>\n",
       "      <td>172</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>520</td>\n",
       "      <td>349</td>\n",
       "      <td>494</td>\n",
       "      <td>54</td>\n",
       "      <td>2923</td>\n",
       "      <td>34</td>\n",
       "      <td>813</td>\n",
       "      <td>136</td>\n",
       "      <td>39</td>\n",
       "      <td>96857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            B_DAT  B_EVE  B_FAC  B_LOC  B_MON  B_ORG  B_PCT  B_PER  B_PRO  \\\n",
       "train        1512   1379   1334  13040    446  15762    266  11371   1719   \n",
       "test          185    218    124   1868     53   2017     27   1566    281   \n",
       "validation    161    143    192   1539     28   2180     33   1335    172   \n",
       "\n",
       "            B_TIM  ...  I_EVE  I_FAC  I_LOC  I_MON  I_ORG  I_PCT  I_PER  \\\n",
       "train         224  ...   4600   2222   4254   1314  21347    308   7160   \n",
       "test           27  ...    697    237    511    142   2843     31   1075   \n",
       "validation     30  ...    520    349    494     54   2923     34    813   \n",
       "\n",
       "            I_PRO  I_TIM       O  \n",
       "train        1736    375  747216  \n",
       "test          345     37   92214  \n",
       "validation    136     39   96857  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633e8d19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Pytorch]",
   "language": "python",
   "name": "conda-env-Pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
