{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac012635",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "train = load_from_disk(\"train_220K.hf\")\n",
    "test = load_from_disk(\"test_220K.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "655f629d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at E:\\ML\\NLP_Toolbox\\NER\\train_220K.hf\\cache-e9109fd02d9e5f0c.arrow\n",
      "Loading cached shuffled indices for dataset at E:\\ML\\NLP_Toolbox\\NER\\test_220K.hf\\cache-41f2bd74d9d4d4e3.arrow\n"
     ]
    }
   ],
   "source": [
    "train = train.shuffle(seed=42)\n",
    "test = test.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28b94699",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from pathlib import Path\n",
    "roberta_model_name = \"HooshvareLab/roberta-fa-zwnj-base\"\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(roberta_model_name,cache_dir=Path.cwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7976c926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at HooshvareLab/roberta-fa-zwnj-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaConfig, RobertaModel\n",
    "from pathlib import Path\n",
    "roberta_model = RobertaModel.from_pretrained(roberta_model_name)\n",
    "roberta_config = RobertaConfig.from_pretrained(roberta_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84607ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel\n",
    "from torch import nn\n",
    "class ParsRobertaForTokenClassification(RobertaPreTrainedModel):\n",
    "    config_class = roberta_config\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        # Load model body\n",
    "        self.parsbert = RobertaModel(config)\n",
    "        # Set up token classification head\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        # Load and initialize weights\n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, **kwargs):\n",
    "        \n",
    "        # Use model body to get encoder representations\n",
    "        outputs = self.parsbert(input_ids, attention_mask=attention_mask,\n",
    "        token_type_ids=token_type_ids, **kwargs)\n",
    "        \n",
    "        # Apply classifier to encoder representation\n",
    "        sequence_output = self.dropout(outputs[0])\n",
    "        logits = self.classifier(sequence_output)\n",
    "        \n",
    "        # Calculate losses\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            \n",
    "        # Return model output object\n",
    "        return TokenClassifierOutput(loss=loss, logits=logits,\n",
    "        hidden_states=outputs.hidden_states,\n",
    "        attentions=outputs.attentions)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3c9ad19",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tags = train[\"ner\"]\n",
    "ner_tag_names = set(tag for tags in ner_tags for tag in tags)\n",
    "                \n",
    "index2tag = {idx: tag for idx, tag in enumerate(ner_tag_names)}\n",
    "tag2index = {tag: idx for idx, tag in enumerate(ner_tag_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efd9c164",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "roberta_config = AutoConfig.from_pretrained(roberta_model_name,num_labels=4,\n",
    "                                            id2label=index2tag, label2id=tag2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc61a6ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>He</td>\n",
       "      <td>Ġl</td>\n",
       "      <td>ater</td>\n",
       "      <td>Ġreturn</td>\n",
       "      <td>ed</td>\n",
       "      <td>Ġto</td>\n",
       "      <td>Ġthe</td>\n",
       "      <td>ĠUnited</td>\n",
       "      <td>ĠSt</td>\n",
       "      <td>ates</td>\n",
       "      <td>Ġwhere</td>\n",
       "      <td>Ġhe</td>\n",
       "      <td>Ġwas</td>\n",
       "      <td>Ġcal</td>\n",
       "      <td>led</td>\n",
       "      <td>Ġin</td>\n",
       "      <td>Ġby</td>\n",
       "      <td>ĠD</td>\n",
       "      <td>.</td>\n",
       "      <td>C</td>\n",
       "      <td>.</td>\n",
       "      <td>ĠUnited</td>\n",
       "      <td>Ġ,</td>\n",
       "      <td>Ġand</td>\n",
       "      <td>Ġplay</td>\n",
       "      <td>ed</td>\n",
       "      <td>Ġse</td>\n",
       "      <td>ver</td>\n",
       "      <td>al</td>\n",
       "      <td>Ġres</td>\n",
       "      <td>erve</td>\n",
       "      <td>Ġg</td>\n",
       "      <td>ames</td>\n",
       "      <td>Ġag</td>\n",
       "      <td>ain</td>\n",
       "      <td>st</td>\n",
       "      <td>ĠCh</td>\n",
       "      <td>ic</td>\n",
       "      <td>ago</td>\n",
       "      <td>ĠFire</td>\n",
       "      <td>Ġ,</td>\n",
       "      <td>ĠNew</td>\n",
       "      <td>ĠEng</td>\n",
       "      <td>land</td>\n",
       "      <td>ĠRev</td>\n",
       "      <td>olution</td>\n",
       "      <td>Ġand</td>\n",
       "      <td>ĠCol</td>\n",
       "      <td>umb</td>\n",
       "      <td>us</td>\n",
       "      <td>ĠCre</td>\n",
       "      <td>w</td>\n",
       "      <td>Ġ,</td>\n",
       "      <td>Ġ.</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Input IDs</th>\n",
       "      <td>0</td>\n",
       "      <td>22606</td>\n",
       "      <td>4486</td>\n",
       "      <td>12480</td>\n",
       "      <td>32617</td>\n",
       "      <td>2156</td>\n",
       "      <td>4738</td>\n",
       "      <td>3497</td>\n",
       "      <td>36300</td>\n",
       "      <td>3700</td>\n",
       "      <td>16267</td>\n",
       "      <td>37988</td>\n",
       "      <td>11742</td>\n",
       "      <td>18536</td>\n",
       "      <td>31629</td>\n",
       "      <td>17145</td>\n",
       "      <td>4142</td>\n",
       "      <td>13474</td>\n",
       "      <td>1410</td>\n",
       "      <td>24</td>\n",
       "      <td>45</td>\n",
       "      <td>24</td>\n",
       "      <td>36300</td>\n",
       "      <td>4221</td>\n",
       "      <td>4517</td>\n",
       "      <td>28918</td>\n",
       "      <td>2156</td>\n",
       "      <td>20796</td>\n",
       "      <td>4823</td>\n",
       "      <td>1353</td>\n",
       "      <td>14426</td>\n",
       "      <td>31058</td>\n",
       "      <td>4998</td>\n",
       "      <td>11323</td>\n",
       "      <td>23277</td>\n",
       "      <td>6005</td>\n",
       "      <td>1731</td>\n",
       "      <td>5475</td>\n",
       "      <td>1699</td>\n",
       "      <td>34133</td>\n",
       "      <td>15592</td>\n",
       "      <td>4221</td>\n",
       "      <td>10445</td>\n",
       "      <td>14437</td>\n",
       "      <td>12630</td>\n",
       "      <td>30916</td>\n",
       "      <td>20077</td>\n",
       "      <td>4517</td>\n",
       "      <td>13055</td>\n",
       "      <td>22420</td>\n",
       "      <td>2831</td>\n",
       "      <td>11040</td>\n",
       "      <td>97</td>\n",
       "      <td>4221</td>\n",
       "      <td>19225</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0      1     2      3        4     5     6     7        8     9   \\\n",
       "Tokens     <s>     He    Ġl   ater  Ġreturn    ed   Ġto  Ġthe  ĠUnited   ĠSt   \n",
       "Input IDs    0  22606  4486  12480    32617  2156  4738  3497    36300  3700   \n",
       "\n",
       "              10      11     12     13     14     15    16     17    18  19  \\\n",
       "Tokens      ates  Ġwhere    Ġhe   Ġwas   Ġcal    led   Ġin    Ġby    ĠD   .   \n",
       "Input IDs  16267   37988  11742  18536  31629  17145  4142  13474  1410  24   \n",
       "\n",
       "           20  21       22    23    24     25    26     27    28    29     30  \\\n",
       "Tokens      C   .  ĠUnited    Ġ,  Ġand  Ġplay    ed    Ġse   ver    al   Ġres   \n",
       "Input IDs  45  24    36300  4221  4517  28918  2156  20796  4823  1353  14426   \n",
       "\n",
       "              31    32     33     34    35    36    37    38     39     40  \\\n",
       "Tokens      erve    Ġg   ames    Ġag   ain    st   ĠCh    ic    ago  ĠFire   \n",
       "Input IDs  31058  4998  11323  23277  6005  1731  5475  1699  34133  15592   \n",
       "\n",
       "             41     42     43     44     45       46    47     48     49  \\\n",
       "Tokens       Ġ,   ĠNew   ĠEng   land   ĠRev  olution  Ġand   ĠCol    umb   \n",
       "Input IDs  4221  10445  14437  12630  30916    20077  4517  13055  22420   \n",
       "\n",
       "             50     51  52    53     54    55  \n",
       "Tokens       us   ĠCre   w    Ġ,     Ġ.  </s>  \n",
       "Input IDs  2831  11040  97  4221  19225     2  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "per_text = \" \".join(train['words'][0])\n",
    "input_ids = roberta_tokenizer.encode(per_text, return_tensors=\"pt\")\n",
    "roberta_tokens = roberta_tokenizer(per_text).tokens()\n",
    "pd.DataFrame([roberta_tokens, input_ids[0].numpy()], index=[\"Tokens\", \"Input IDs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13fe513e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "roberta_tokenizer = RobertaTokenizerFast.from_pretrained(roberta_model_name, add_prefix_space=True)\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = roberta_tokenizer(examples[\"words\"], truncation=True,\n",
    "    is_split_into_words=True)\n",
    "    labels = []\n",
    "    for idx, label in enumerate(examples[\"ner\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None or word_idx == previous_word_idx:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_token = label[word_idx]\n",
    "                # Use the label map to get the numerical value for each entity\n",
    "                label_ids.append(tag2index[label_token])\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "642b77a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_panx_dataset(corpus):\n",
    "    return corpus.map(tokenize_and_align_labels,batched=True,\n",
    "                      remove_columns=['id','words','lang','ner','ratio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "172f22f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/171745 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/42937 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_encoded = encode_panx_dataset(train)\n",
    "test_encoded = encode_panx_dataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf1f338d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "roberta_config = AutoConfig.from_pretrained(roberta_model_name,num_labels=4,\n",
    "                                            id2label=index2tag, label2id=tag2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab4d2ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 171745\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d4f695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def align_predictions(predictions, label_ids):\n",
    "        preds = np.argmax(predictions, axis=2)\n",
    "        batch_size, seq_len = preds.shape\n",
    "        labels_list, preds_list = [], []\n",
    "        for batch_idx in range(batch_size):\n",
    "            example_labels, example_preds = [], []\n",
    "            for seq_idx in range(seq_len):\n",
    "                # Ignore label IDs = -100\n",
    "                if label_ids[batch_idx, seq_idx] != -100:\n",
    "                    example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])\n",
    "                    example_preds.append(index2tag[preds[batch_idx][seq_idx]])\n",
    "            labels_list.append(example_labels)\n",
    "            preds_list.append(example_preds)\n",
    "        return preds_list, labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4b33bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "import torch\n",
    "\n",
    "num_epochs = 2\n",
    "batch_size = 18\n",
    "logging_steps = len(train_encoded) \n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output\", log_level=\"error\", num_train_epochs=num_epochs,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    optim=\"adafactor\",\n",
    "    eval_accumulation_steps=10,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=18,\n",
    "    seed=42,\n",
    "    logging_strategy=\"steps\", evaluation_strategy=\"epoch\",\n",
    "    save_steps=1e6, weight_decay=0.01, disable_tqdm=False,\n",
    "    logging_steps=logging_steps, push_to_hub=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00aef084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return (ParsRobertaForTokenClassification\n",
    "                  .from_pretrained(roberta_model_name, config=roberta_config,cache_dir=Path.cwd())\n",
    "                  .to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72532ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "class CustomDataCollator(DataCollatorForTokenClassification):\n",
    "    def __call__(self, features):\n",
    "        batch = super().__call__(features)\n",
    "        # Filter out the '-100' label IDs (padding tokens)\n",
    "        batch[\"labels\"] = torch.where(batch[\"labels\"] != -100, batch[\"labels\"], -100)\n",
    "        return batch\n",
    "\n",
    "# Use the custom data collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=roberta_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84b42146",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    y_pred, y_true = align_predictions(eval_pred.predictions,\n",
    "    eval_pred.label_ids)\n",
    "    return {\"f1\": f1_score(y_true, y_pred)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "acdd94a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "import torch\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "trainer = Trainer(model_init=model_init, args=training_args,\n",
    "                    data_collator=data_collator, compute_metrics=compute_metrics,\n",
    "                    train_dataset=train_encoded,\n",
    "                    eval_dataset=test_encoded,\n",
    "                    tokenizer=roberta_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca5b350b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10736' max='10736' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10736/10736 36:20, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.177999</td>\n",
       "      <td>0.592254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.152724</td>\n",
       "      <td>0.660872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: LOC seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\11\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ORG seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\11\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: PER seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    }
   ],
   "source": [
    "result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "597ace28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51711860",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Pytorch]",
   "language": "python",
   "name": "conda-env-Pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
