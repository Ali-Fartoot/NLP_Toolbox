{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ac012635",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "train = load_from_disk(\"train_25k.hf\")\n",
    "test = load_from_disk(\"train_25k.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "655f629d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at E:\\ML\\NLP_Toolbox\\NER\\train.hf\\cache-aab69560c24eae2b.arrow\n",
      "Loading cached shuffled indices for dataset at E:\\ML\\NLP_Toolbox\\NER\\test.hf\\cache-6c7c52e5435094d8.arrow\n"
     ]
    }
   ],
   "source": [
    "train = train.shuffle(seed=42)\n",
    "test = test.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28b94699",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from pathlib import Path\n",
    "roberta_model_name = \"HooshvareLab/roberta-fa-zwnj-base\"\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(roberta_model_name,cache_dir=Path.cwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7976c926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at HooshvareLab/roberta-fa-zwnj-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaConfig, RobertaModel\n",
    "from pathlib import Path\n",
    "roberta_model = RobertaModel.from_pretrained(roberta_model_name)\n",
    "roberta_config = RobertaConfig.from_pretrained(roberta_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84607ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel\n",
    "from torch import nn\n",
    "class ParsRobertaForTokenClassification(RobertaPreTrainedModel):\n",
    "    config_class = roberta_config\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        # Load model body\n",
    "        self.parsbert = RobertaModel(config)\n",
    "        # Set up token classification head\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        # Load and initialize weights\n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, **kwargs):\n",
    "        \n",
    "        # Use model body to get encoder representations\n",
    "        outputs = self.parsbert(input_ids, attention_mask=attention_mask,\n",
    "        token_type_ids=token_type_ids, **kwargs)\n",
    "        \n",
    "        # Apply classifier to encoder representation\n",
    "        sequence_output = self.dropout(outputs[0])\n",
    "        logits = self.classifier(sequence_output)\n",
    "        \n",
    "        # Calculate losses\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            \n",
    "        # Return model output object\n",
    "        return TokenClassifierOutput(loss=loss, logits=logits,\n",
    "        hidden_states=outputs.hidden_states,\n",
    "        attentions=outputs.attentions)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3c9ad19",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tags = train[\"ner\"]\n",
    "ner_tag_names = set(tag for tags in ner_tags for tag in tags)\n",
    "                \n",
    "index2tag = {idx: tag for idx, tag in enumerate(ner_tag_names)}\n",
    "tag2index = {tag: idx for idx, tag in enumerate(ner_tag_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efd9c164",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "roberta_config = AutoConfig.from_pretrained(roberta_model_name,num_labels=4,\n",
    "                                            id2label=index2tag, label2id=tag2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc61a6ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>U</td>\n",
       "      <td>ers</td>\n",
       "      <td>us</td>\n",
       "      <td>ĠO</td>\n",
       "      <td>ro</td>\n",
       "      <td>d</td>\n",
       "      <td>in</td>\n",
       "      <td>um</td>\n",
       "      <td>Ġet</td>\n",
       "      <td>ĠCh</td>\n",
       "      <td>and</td>\n",
       "      <td>in</td>\n",
       "      <td>um</td>\n",
       "      <td>Ġpro</td>\n",
       "      <td>ced</td>\n",
       "      <td>ere</td>\n",
       "      <td>Ġin</td>\n",
       "      <td>ce</td>\n",
       "      <td>per</td>\n",
       "      <td>unt</td>\n",
       "      <td>Ġ,</td>\n",
       "      <td>Ġin</td>\n",
       "      <td>Ġmed</td>\n",
       "      <td>io</td>\n",
       "      <td>Ġe</td>\n",
       "      <td>or</td>\n",
       "      <td>um</td>\n",
       "      <td>Ġ,</td>\n",
       "      <td>ĠN</td>\n",
       "      <td>ou</td>\n",
       "      <td>a</td>\n",
       "      <td>-</td>\n",
       "      <td>u</td>\n",
       "      <td>ill</td>\n",
       "      <td>i</td>\n",
       "      <td>Ġd</td>\n",
       "      <td>im</td>\n",
       "      <td>iss</td>\n",
       "      <td>a</td>\n",
       "      <td>Ġ,</td>\n",
       "      <td>Ġn</td>\n",
       "      <td>omen</td>\n",
       "      <td>Ġc</td>\n",
       "      <td>u</td>\n",
       "      <td>ius</td>\n",
       "      <td>ĠPer</td>\n",
       "      <td>eg</td>\n",
       "      <td>Ġf</td>\n",
       "      <td>u</td>\n",
       "      <td>er</td>\n",
       "      <td>at</td>\n",
       "      <td>Ġ,</td>\n",
       "      <td>Ġin</td>\n",
       "      <td>Ġqu</td>\n",
       "      <td>a</td>\n",
       "      <td>Ġh</td>\n",
       "      <td>om</td>\n",
       "      <td>ines</td>\n",
       "      <td>Ġs</td>\n",
       "      <td>ept</td>\n",
       "      <td>u</td>\n",
       "      <td>ag</td>\n",
       "      <td>int</td>\n",
       "      <td>a</td>\n",
       "      <td>Ġu</td>\n",
       "      <td>ill</td>\n",
       "      <td>ar</td>\n",
       "      <td>um</td>\n",
       "      <td>Ġf</td>\n",
       "      <td>u</td>\n",
       "      <td>er</td>\n",
       "      <td>ant</td>\n",
       "      <td>Ġc</td>\n",
       "      <td>ong</td>\n",
       "      <td>reg</td>\n",
       "      <td>ati</td>\n",
       "      <td>Ġ,</td>\n",
       "      <td>Ġet</td>\n",
       "      <td>Ġd</td>\n",
       "      <td>im</td>\n",
       "      <td>iss</td>\n",
       "      <td>o</td>\n",
       "      <td>Ġmon</td>\n",
       "      <td>aster</td>\n",
       "      <td>io</td>\n",
       "      <td>ĠE</td>\n",
       "      <td>g</td>\n",
       "      <td>res</td>\n",
       "      <td>ĠC</td>\n",
       "      <td>ister</td>\n",
       "      <td>ci</td>\n",
       "      <td>ens</td>\n",
       "      <td>is</td>\n",
       "      <td>Ġ</td>\n",
       "      <td>ord</td>\n",
       "      <td>in</td>\n",
       "      <td>is</td>\n",
       "      <td>Ġ,</td>\n",
       "      <td>Ġin</td>\n",
       "      <td>Ġqu</td>\n",
       "      <td>od</td>\n",
       "      <td>Ġt</td>\n",
       "      <td>an</td>\n",
       "      <td>qu</td>\n",
       "      <td>am</td>\n",
       "      <td>Ġm</td>\n",
       "      <td>un</td>\n",
       "      <td>it</td>\n",
       "      <td>um</td>\n",
       "      <td>Ġc</td>\n",
       "      <td>ast</td>\n",
       "      <td>rum</td>\n",
       "      <td>Ġse</td>\n",
       "      <td>Ġm</td>\n",
       "      <td>il</td>\n",
       "      <td>ites</td>\n",
       "      <td>Ġet</td>\n",
       "      <td>Ġmult</td>\n",
       "      <td>e</td>\n",
       "      <td>Ġd</td>\n",
       "      <td>om</td>\n",
       "      <td>ine</td>\n",
       "      <td>Ġre</td>\n",
       "      <td>ce</td>\n",
       "      <td>per</td>\n",
       "      <td>ant</td>\n",
       "      <td>Ġ.</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Input IDs</th>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>4219</td>\n",
       "      <td>2831</td>\n",
       "      <td>2083</td>\n",
       "      <td>1692</td>\n",
       "      <td>78</td>\n",
       "      <td>949</td>\n",
       "      <td>3751</td>\n",
       "      <td>23590</td>\n",
       "      <td>5475</td>\n",
       "      <td>2885</td>\n",
       "      <td>949</td>\n",
       "      <td>3751</td>\n",
       "      <td>6467</td>\n",
       "      <td>28654</td>\n",
       "      <td>11916</td>\n",
       "      <td>4142</td>\n",
       "      <td>3707</td>\n",
       "      <td>4682</td>\n",
       "      <td>16346</td>\n",
       "      <td>4221</td>\n",
       "      <td>4142</td>\n",
       "      <td>37180</td>\n",
       "      <td>6232</td>\n",
       "      <td>6088</td>\n",
       "      <td>1172</td>\n",
       "      <td>3751</td>\n",
       "      <td>4221</td>\n",
       "      <td>1881</td>\n",
       "      <td>2088</td>\n",
       "      <td>75</td>\n",
       "      <td>23</td>\n",
       "      <td>95</td>\n",
       "      <td>6078</td>\n",
       "      <td>83</td>\n",
       "      <td>3094</td>\n",
       "      <td>3515</td>\n",
       "      <td>12046</td>\n",
       "      <td>75</td>\n",
       "      <td>4221</td>\n",
       "      <td>4120</td>\n",
       "      <td>36980</td>\n",
       "      <td>2578</td>\n",
       "      <td>95</td>\n",
       "      <td>21077</td>\n",
       "      <td>14288</td>\n",
       "      <td>7048</td>\n",
       "      <td>2927</td>\n",
       "      <td>95</td>\n",
       "      <td>837</td>\n",
       "      <td>1276</td>\n",
       "      <td>4221</td>\n",
       "      <td>4142</td>\n",
       "      <td>16492</td>\n",
       "      <td>75</td>\n",
       "      <td>4472</td>\n",
       "      <td>2221</td>\n",
       "      <td>24707</td>\n",
       "      <td>2529</td>\n",
       "      <td>31578</td>\n",
       "      <td>95</td>\n",
       "      <td>2963</td>\n",
       "      <td>7689</td>\n",
       "      <td>75</td>\n",
       "      <td>19506</td>\n",
       "      <td>6078</td>\n",
       "      <td>1201</td>\n",
       "      <td>3751</td>\n",
       "      <td>2927</td>\n",
       "      <td>95</td>\n",
       "      <td>837</td>\n",
       "      <td>5744</td>\n",
       "      <td>2578</td>\n",
       "      <td>8682</td>\n",
       "      <td>24941</td>\n",
       "      <td>35610</td>\n",
       "      <td>4221</td>\n",
       "      <td>23590</td>\n",
       "      <td>3094</td>\n",
       "      <td>3515</td>\n",
       "      <td>12046</td>\n",
       "      <td>89</td>\n",
       "      <td>35230</td>\n",
       "      <td>13078</td>\n",
       "      <td>6232</td>\n",
       "      <td>1696</td>\n",
       "      <td>81</td>\n",
       "      <td>13190</td>\n",
       "      <td>1040</td>\n",
       "      <td>27467</td>\n",
       "      <td>30634</td>\n",
       "      <td>7841</td>\n",
       "      <td>2433</td>\n",
       "      <td>231</td>\n",
       "      <td>6543</td>\n",
       "      <td>949</td>\n",
       "      <td>2433</td>\n",
       "      <td>4221</td>\n",
       "      <td>4142</td>\n",
       "      <td>16492</td>\n",
       "      <td>3591</td>\n",
       "      <td>1753</td>\n",
       "      <td>1105</td>\n",
       "      <td>6240</td>\n",
       "      <td>2350</td>\n",
       "      <td>2807</td>\n",
       "      <td>3171</td>\n",
       "      <td>1582</td>\n",
       "      <td>3751</td>\n",
       "      <td>2578</td>\n",
       "      <td>5345</td>\n",
       "      <td>40186</td>\n",
       "      <td>20796</td>\n",
       "      <td>2807</td>\n",
       "      <td>2544</td>\n",
       "      <td>27639</td>\n",
       "      <td>23590</td>\n",
       "      <td>27793</td>\n",
       "      <td>79</td>\n",
       "      <td>3094</td>\n",
       "      <td>2221</td>\n",
       "      <td>5372</td>\n",
       "      <td>5750</td>\n",
       "      <td>3707</td>\n",
       "      <td>4682</td>\n",
       "      <td>5744</td>\n",
       "      <td>19225</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0   1     2     3     4     5   6    7     8      9     10    11   \\\n",
       "Tokens     <s>   U   ers    us    ĠO    ro   d   in    um    Ġet   ĠCh   and   \n",
       "Input IDs    0  63  4219  2831  2083  1692  78  949  3751  23590  5475  2885   \n",
       "\n",
       "           12    13    14     15     16    17    18    19     20    21    22   \\\n",
       "Tokens      in    um  Ġpro    ced    ere   Ġin    ce   per    unt    Ġ,   Ġin   \n",
       "Input IDs  949  3751  6467  28654  11916  4142  3707  4682  16346  4221  4142   \n",
       "\n",
       "             23    24    25    26    27    28    29    30  31  32  33    34   \\\n",
       "Tokens      Ġmed    io    Ġe    or    um    Ġ,    ĠN    ou   a   -   u   ill   \n",
       "Input IDs  37180  6232  6088  1172  3751  4221  1881  2088  75  23  95  6078   \n",
       "\n",
       "          35    36    37     38  39    40    41     42    43  44     45   \\\n",
       "Tokens      i    Ġd    im    iss   a    Ġ,    Ġn   omen    Ġc   u    ius   \n",
       "Input IDs  83  3094  3515  12046  75  4221  4120  36980  2578  95  21077   \n",
       "\n",
       "             46    47    48  49   50    51    52    53     54  55    56   \\\n",
       "Tokens      ĠPer    eg    Ġf   u   er    at    Ġ,   Ġin    Ġqu   a    Ġh   \n",
       "Input IDs  14288  7048  2927  95  837  1276  4221  4142  16492  75  4472   \n",
       "\n",
       "            57     58    59     60  61    62    63  64     65    66    67   \\\n",
       "Tokens       om   ines    Ġs    ept   u    ag   int   a     Ġu   ill    ar   \n",
       "Input IDs  2221  24707  2529  31578  95  2963  7689  75  19506  6078  1201   \n",
       "\n",
       "            68    69  70   71    72    73    74     75     76    77     78   \\\n",
       "Tokens       um    Ġf   u   er   ant    Ġc   ong    reg    ati    Ġ,    Ġet   \n",
       "Input IDs  3751  2927  95  837  5744  2578  8682  24941  35610  4221  23590   \n",
       "\n",
       "            79    80     81  82     83     84    85    86  87     88    89   \\\n",
       "Tokens       Ġd    im    iss   o   Ġmon  aster    io    ĠE   g    res    ĠC   \n",
       "Input IDs  3094  3515  12046  89  35230  13078  6232  1696  81  13190  1040   \n",
       "\n",
       "             90     91    92    93   94    95   96    97    98    99     100  \\\n",
       "Tokens     ister     ci   ens    is    Ġ   ord   in    is    Ġ,   Ġin    Ġqu   \n",
       "Input IDs  27467  30634  7841  2433  231  6543  949  2433  4221  4142  16492   \n",
       "\n",
       "            101   102   103   104   105   106   107   108   109   110   111  \\\n",
       "Tokens       od    Ġt    an    qu    am    Ġm    un    it    um    Ġc   ast   \n",
       "Input IDs  3591  1753  1105  6240  2350  2807  3171  1582  3751  2578  5345   \n",
       "\n",
       "             112    113   114   115    116    117    118 119   120   121  \\\n",
       "Tokens       rum    Ġse    Ġm    il   ites    Ġet  Ġmult   e    Ġd    om   \n",
       "Input IDs  40186  20796  2807  2544  27639  23590  27793  79  3094  2221   \n",
       "\n",
       "            122   123   124   125   126    127   128  \n",
       "Tokens      ine   Ġre    ce   per   ant     Ġ.  </s>  \n",
       "Input IDs  5372  5750  3707  4682  5744  19225     2  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "per_text = \" \".join(train['words'][0])\n",
    "input_ids = roberta_tokenizer.encode(per_text, return_tensors=\"pt\")\n",
    "roberta_tokens = roberta_tokenizer(per_text).tokens()\n",
    "pd.DataFrame([roberta_tokens, input_ids[0].numpy()], index=[\"Tokens\", \"Input IDs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "13fe513e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(roberta_model_name, add_prefix_space=True)\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = roberta_tokenizer(examples[\"words\"], truncation=True,\n",
    "    is_split_into_words=True)\n",
    "    labels = []\n",
    "    for idx, label in enumerate(examples[\"ner\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None or word_idx == previous_word_idx:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_token = label[word_idx]\n",
    "                # Use the label map to get the numerical value for each entity\n",
    "                label_ids.append(tag2index[label_token])\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "642b77a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_panx_dataset(corpus):\n",
    "    return corpus.map(tokenize_and_align_labels,batched=True,\n",
    "                      remove_columns=['id','words','lang','ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "172f22f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_encoded = encode_panx_dataset(train)\n",
    "test_encoded = encode_panx_dataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf1f338d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "roberta_config = AutoConfig.from_pretrained(roberta_model_name,num_labels=4,\n",
    "                                            id2label=index2tag, label2id=tag2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab4d2ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 500000\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d4f695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def align_predictions(predictions, label_ids):\n",
    "        preds = np.argmax(predictions, axis=2)\n",
    "        batch_size, seq_len = preds.shape\n",
    "        labels_list, preds_list = [], []\n",
    "        for batch_idx in range(batch_size):\n",
    "            example_labels, example_preds = [], []\n",
    "            for seq_idx in range(seq_len):\n",
    "                # Ignore label IDs = -100\n",
    "                if label_ids[batch_idx, seq_idx] != -100:\n",
    "                    example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])\n",
    "                    example_preds.append(index2tag[preds[batch_idx][seq_idx]])\n",
    "            labels_list.append(example_labels)\n",
    "            preds_list.append(example_preds)\n",
    "        return preds_list, labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4b33bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "import torch\n",
    "\n",
    "num_epochs = 2\n",
    "batch_size = 18\n",
    "logging_steps = len(train_encoded) \n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output\", log_level=\"error\", num_train_epochs=num_epochs,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    optim=\"adafactor\",\n",
    "    eval_accumulation_steps=10,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=18,\n",
    "    learning_rate = 1e-4,\n",
    "    seed=42,\n",
    "    logging_strategy=\"steps\", evaluation_strategy=\"epoch\",\n",
    "    save_steps=1e6, weight_decay=0.01, disable_tqdm=False,\n",
    "    logging_steps=logging_steps, push_to_hub=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00aef084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return (ParsRobertaForTokenClassification\n",
    "                  .from_pretrained(roberta_model_name, config=roberta_config,cache_dir=Path.cwd())\n",
    "                  .to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72532ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "class CustomDataCollator(DataCollatorForTokenClassification):\n",
    "    def __call__(self, features):\n",
    "        batch = super().__call__(features)\n",
    "        # Filter out the '-100' label IDs (padding tokens)\n",
    "        batch[\"labels\"] = torch.where(batch[\"labels\"] != -100, batch[\"labels\"], -100)\n",
    "        return batch\n",
    "\n",
    "# Use the custom data collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=roberta_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84b42146",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    y_pred, y_true = align_predictions(eval_pred.predictions,\n",
    "    eval_pred.label_ids)\n",
    "    return {\"f1\": f1_score(y_true, y_pred)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "acdd94a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "import torch\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "trainer = Trainer(model_init=model_init, args=training_args,\n",
    "                    data_collator=data_collator, compute_metrics=compute_metrics,\n",
    "                    train_dataset=train_encoded,\n",
    "                    eval_dataset=test_encoded,\n",
    "                    tokenizer=roberta_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca5b350b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31250' max='31250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [31250/31250 1:20:11, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.092580</td>\n",
       "      <td>0.576666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.082258</td>\n",
       "      <td>0.618711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ORG seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\11\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: PER seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\11\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: LOC seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    }
   ],
   "source": [
    "result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "78aa5957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>ĠJack</td>\n",
       "      <td>ĠSp</td>\n",
       "      <td>ar</td>\n",
       "      <td>row</td>\n",
       "      <td>Ġl</td>\n",
       "      <td>ov</td>\n",
       "      <td>es</td>\n",
       "      <td>ĠNew</td>\n",
       "      <td>ĠYork</td>\n",
       "      <td>!</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>PER</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>LOC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1    2   3    4   5   6   7     8      9    10    11\n",
       "Tokens  <s>  ĠJack  ĠSp  ar  row  Ġl  ov  es  ĠNew  ĠYork    !  </s>\n",
       "Tags      O      O    O   O  PER   O   O   O     O      O  LOC     O"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_de = \"Jack Sparrow loves New York!\"\n",
    "tokens = roberta_tokenizer(text_de).tokens()\n",
    "input_ids = roberta_tokenizer(text_de, return_tensors=\"pt\").input_ids.to(device)\n",
    "outputs = trainer.model(input_ids)[0]\n",
    "# Take argmax to get most likely class per token\n",
    "predictions = torch.argmax(outputs, dim=2)\n",
    "# Convert to DataFrame\n",
    "tags = list(ner_tag_names)\n",
    "preds = [tags[p] for p in predictions[0].cpu().numpy()]\n",
    "pd.DataFrame([tokens, preds], index=[\"Tokens\", \"Tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbe7e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Pytorch]",
   "language": "python",
   "name": "conda-env-Pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
