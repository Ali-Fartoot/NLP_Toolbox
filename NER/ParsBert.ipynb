{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ca98b0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "train = load_from_disk(\"train_25k.hf\")\n",
    "test = load_from_disk(\"test_25k.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7cc83052",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at E:\\ML\\NLP_Toolbox\\NER\\train_25k.hf\\cache-fb1c627d3ac8e13f.arrow\n",
      "Loading cached shuffled indices for dataset at E:\\ML\\NLP_Toolbox\\NER\\test_25k.hf\\cache-52664ed69309d356.arrow\n"
     ]
    }
   ],
   "source": [
    "train = train.shuffle(seed=42)\n",
    "test = test.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "932b9c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "\n",
    "# Get the total number of rows in the dataset\n",
    "total_rows = len(test)\n",
    "\n",
    "# Set the number of rows you want to select\n",
    "num_rows_to_select = 2000\n",
    "\n",
    "# Generate a list of random indices without duplicates\n",
    "random_indices = random.sample(range(total_rows), num_rows_to_select)\n",
    "\n",
    "# Select the rows based on the random indices\n",
    "test = test.select(random_indices)\n",
    "\n",
    "# Now `selected_rows` contains your randomly selected 2000 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "70a7f592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'lang', 'words', 'ner', 'ratio'],\n",
       "    num_rows: 2000\n",
       "})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "27cc7712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jack', 'Sp', '##ar', '##row', 'love', '##s', 'New', 'York', '!']\n",
      "['سلام', 'خوب', 'هستید', '؟']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from pathlib import Path\n",
    "parsbert_model_name = \"HooshvareLab/bert-fa-zwnj-base\"\n",
    "parsbert_tokenizer = AutoTokenizer.from_pretrained(parsbert_model_name,cache_dir=Path.cwd())\n",
    "\n",
    "text = \"Jack Sparrow loves New York!\"\n",
    "print(parsbert_tokenizer.tokenize(text))\n",
    "\n",
    "text = \"سلام خوب هستید؟\"\n",
    "print(parsbert_tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e2547da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoConfig\n",
    "from pathlib import Path\n",
    "parsbert_model = AutoModel.from_pretrained(parsbert_model_name,cache_dir=Path.cwd())\n",
    "parsbert_config = AutoConfig.from_pretrained(parsbert_model_name,cache_dir=Path.cwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4ea3d5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers.models.bert.modeling_bert import BertPreTrainedModel,BertModel \n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class CustomBertModel(BertModel):\n",
    "    def __init__(self, config):\n",
    "        super(CustomBertModel, self).__init__(config)\n",
    "        # Remove the pooler part\n",
    "        self.pooler = None\n",
    "\n",
    "# Load the pre-trained BERT model\n",
    "model_name = \"HooshvareLab/bert-fa-zwnj-base\"  # Replace with the appropriate model name\n",
    "config = AutoConfig.from_pretrained(model_name,cache_dir=Path.cwd())\n",
    "\n",
    "\n",
    "class ParsBertForTokenClassification(BertPreTrainedModel):\n",
    "    config_class = config\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        # Load model body\n",
    "        self.parsbert = CustomBertModel(config)\n",
    "        # Set up token classification head\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        # Load and initialize weights\n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, **kwargs):\n",
    "        \n",
    "        # Use model body to get encoder representations\n",
    "        outputs = self.parsbert(input_ids, attention_mask=attention_mask,\n",
    "        token_type_ids=token_type_ids, **kwargs)\n",
    "        \n",
    "        # Apply classifier to encoder representation\n",
    "        sequence_output = self.dropout(outputs[0])\n",
    "        logits = self.classifier(sequence_output)\n",
    "        \n",
    "        # Calculate losses\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            \n",
    "        # Return model output object\n",
    "        return TokenClassifierOutput(loss=loss, logits=logits,\n",
    "        hidden_states=outputs.hidden_states,\n",
    "        attentions=outputs.attentions)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dfd4cafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tags = train[\"ner\"]\n",
    "ner_tag_names = set(tag for tags in ner_tags for tag in tags)\n",
    "                \n",
    "index2tag = {idx: tag for idx, tag in enumerate(ner_tag_names)}\n",
    "tag2index = {tag: idx for idx, tag in enumerate(ner_tag_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "49f5a122",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "parsbert_config = AutoConfig.from_pretrained(parsbert_model_name,num_labels=4,\n",
    "                                            id2label=index2tag, label2id=tag2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "86e386aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>It</td>\n",
       "      <td>was</td>\n",
       "      <td>rel</td>\n",
       "      <td>##e</td>\n",
       "      <td>##ase</td>\n",
       "      <td>##d</td>\n",
       "      <td>on</td>\n",
       "      <td>October</td>\n",
       "      <td>6</td>\n",
       "      <td>,</td>\n",
       "      <td>2003</td>\n",
       "      <td>.</td>\n",
       "      <td>[SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Input IDs</th>\n",
       "      <td>2</td>\n",
       "      <td>16159</td>\n",
       "      <td>20588</td>\n",
       "      <td>25308</td>\n",
       "      <td>1145</td>\n",
       "      <td>10814</td>\n",
       "      <td>1159</td>\n",
       "      <td>9384</td>\n",
       "      <td>40002</td>\n",
       "      <td>129</td>\n",
       "      <td>119</td>\n",
       "      <td>22323</td>\n",
       "      <td>121</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0      1      2      3     4      5     6     7        8    9   \\\n",
       "Tokens     [CLS]     It    was    rel   ##e  ##ase   ##d    on  October    6   \n",
       "Input IDs      2  16159  20588  25308  1145  10814  1159  9384    40002  129   \n",
       "\n",
       "            10     11   12     13  \n",
       "Tokens       ,   2003    .  [SEP]  \n",
       "Input IDs  119  22323  121      3  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "per_text = \" \".join(train['words'][0])\n",
    "input_ids = parsbert_tokenizer.encode(per_text, return_tensors=\"pt\")\n",
    "bert_tokens = parsbert_tokenizer(per_text).tokens()\n",
    "pd.DataFrame([bert_tokens, input_ids[0].numpy()], index=[\"Tokens\", \"Input IDs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b2ed4613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "parsbert_tokenizer = AutoTokenizer.from_pretrained(parsbert_model_name,\n",
    "                                                   use_fast = True,\n",
    "                                                   add_prefix_space=True)\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = parsbert_tokenizer(examples[\"words\"], truncation=True,\n",
    "    is_split_into_words=True)\n",
    "    labels = []\n",
    "    for idx, label in enumerate(examples[\"ner\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None or word_idx == previous_word_idx:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_token = label[word_idx]\n",
    "                # Use the label map to get the numerical value for each entity\n",
    "                label_ids.append(tag2index[label_token])\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e766c759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_panx_dataset(corpus):\n",
    "    return corpus.map(tokenize_and_align_labels,batched=True,\n",
    "                      remove_columns=['id','words','lang','ner','ratio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5975f0e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_encoded = encode_panx_dataset(train)\n",
    "test_encoded = encode_panx_dataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "10c8ee28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 20000\n",
       "})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9fea2415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 2000\n",
       "})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ff8ee7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def align_predictions(predictions, label_ids):\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "    batch_size, seq_len = preds.shape\n",
    "    labels_list, preds_list = [], []\n",
    "\n",
    "    for batch_idx in range(batch_size):\n",
    "        example_labels, example_preds = [], []\n",
    "        for seq_idx in range(seq_len):\n",
    "            # Ignore label IDs = -100\n",
    "            if label_ids[batch_idx, seq_idx] != -100:\n",
    "                example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])\n",
    "                example_preds.append(index2tag[preds[batch_idx][seq_idx]])\n",
    "\n",
    "        labels_list.append(example_labels)\n",
    "        preds_list.append(example_preds)\n",
    "\n",
    "    return preds_list, labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4d015409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return (ParsBertForTokenClassification\n",
    "                  .from_pretrained(parsbert_model_name, config=parsbert_config,cache_dir=Path.cwd())\n",
    "                  .to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d8957fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "import torch\n",
    "\n",
    "num_epochs = 3\n",
    "batch_size = 24\n",
    "\n",
    "logging_steps = len(train_encoded) // batch_size\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output\", log_level=\"error\", num_train_epochs=num_epochs,\n",
    "    gradient_checkpointing=True,\n",
    "#     fp16=True,\n",
    "    eval_accumulation_steps=10,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    seed=42,\n",
    "    logging_strategy=\"steps\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_steps=1e6,\n",
    "    weight_decay=0.01,\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=logging_steps, push_to_hub=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4dbc909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "class CustomDataCollator(DataCollatorForTokenClassification):\n",
    "    def __call__(self, features):\n",
    "        batch = super().__call__(features)\n",
    "        # Filter out the '-100' label IDs (padding tokens)\n",
    "        batch[\"labels\"] = torch.where(batch[\"labels\"] != -100, batch[\"labels\"], -100)\n",
    "        return batch\n",
    "\n",
    "# Use the custom data collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=parsbert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6bde6261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score,recall_score,precision_score,accuracy_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    y_pred, y_true = align_predictions(eval_pred.predictions,\n",
    "    eval_pred.label_ids)\n",
    "    return {\"f1\": f1_score(y_true, y_pred),\"Recall\":recall_score(y_true, y_pred),\"Precision\":precision_score(y_true, y_pred),\"Accuracy\":accuracy_score(y_true, y_pred)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "86926f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2502' max='2502' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2502/2502 07:21, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.307400</td>\n",
       "      <td>0.227603</td>\n",
       "      <td>0.374849</td>\n",
       "      <td>0.327696</td>\n",
       "      <td>0.437853</td>\n",
       "      <td>0.915770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.209800</td>\n",
       "      <td>0.204557</td>\n",
       "      <td>0.472063</td>\n",
       "      <td>0.453700</td>\n",
       "      <td>0.491976</td>\n",
       "      <td>0.923104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.172600</td>\n",
       "      <td>0.196831</td>\n",
       "      <td>0.503502</td>\n",
       "      <td>0.501480</td>\n",
       "      <td>0.505541</td>\n",
       "      <td>0.926503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: LOC seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\11\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ORG seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\11\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: PER seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2502, training_loss=0.22982711100178085, metrics={'train_runtime': 441.464, 'train_samples_per_second': 135.911, 'train_steps_per_second': 5.668, 'total_flos': 3199416167499648.0, 'train_loss': 0.22982711100178085, 'epoch': 3.0})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "training_args.set_optimizer(learning_rate=5e-05,epsilon=1e-08)\n",
    "# print(training_args)\n",
    "trainer = Trainer(model_init=model_init, args=training_args,\n",
    "                        data_collator=data_collator, compute_metrics=compute_metrics,\n",
    "                        train_dataset=train_encoded,\n",
    "                        eval_dataset=test_encoded,\n",
    "                        tokenizer=parsbert_tokenizer)\n",
    "trainer.train()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d2ae0551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>در</td>\n",
       "      <td>سال</td>\n",
       "      <td>۲۰۱۳</td>\n",
       "      <td>درگذشت</td>\n",
       "      <td>و</td>\n",
       "      <td>آ</td>\n",
       "      <td>##ندر</td>\n",
       "      <td>##تیک</td>\n",
       "      <td>##ر</td>\n",
       "      <td>و</td>\n",
       "      <td>کین</td>\n",
       "      <td>برای</td>\n",
       "      <td>او</td>\n",
       "      <td>مراسم</td>\n",
       "      <td>یادبود</td>\n",
       "      <td>گرفتند</td>\n",
       "      <td>.</td>\n",
       "      <td>[SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0   1    2     3       4  5  6      7      8    9  10   11    12  \\\n",
       "Tokens  [CLS]  در  سال  ۲۰۱۳  درگذشت  و  آ  ##ندر  ##تیک  ##ر  و  کین  برای   \n",
       "Tags        O   O    O     O       O  O  O      O      O    O  O    O     O   \n",
       "\n",
       "        13     14      15      16 17     18  \n",
       "Tokens  او  مراسم  یادبود  گرفتند  .  [SEP]  \n",
       "Tags     O      O       O       O  O      O  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = \"در سال ۲۰۱۳ درگذشت و آندرتیکر و کین برای او مراسم یادبود گرفتند.\"\n",
    "tokens = parsbert_tokenizer(example).tokens()\n",
    "input_ids = parsbert_tokenizer(example, return_tensors=\"pt\").input_ids.to(device)\n",
    "outputs = trainer.model(input_ids)[0]\n",
    "# Take argmax to get most likely class per token\n",
    "predictions = torch.argmax(outputs, dim=2)\n",
    "# Convert to DataFrame\n",
    "tags = list(ner_tag_names)\n",
    "preds = [tags[p] for p in predictions[0].cpu().numpy()]\n",
    "pd.DataFrame([tokens, preds], index=[\"Tokens\", \"Tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e11f4fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>این</td>\n",
       "      <td>مجله</td>\n",
       "      <td>با</td>\n",
       "      <td>نگاهی</td>\n",
       "      <td>تحلیلی</td>\n",
       "      <td>به</td>\n",
       "      <td>رویدادها</td>\n",
       "      <td>و</td>\n",
       "      <td>جریان</td>\n",
       "      <td>[ZWNJ]</td>\n",
       "      <td>های</td>\n",
       "      <td>مهم</td>\n",
       "      <td>موسیقی</td>\n",
       "      <td>ایران</td>\n",
       "      <td>و</td>\n",
       "      <td>جهان</td>\n",
       "      <td>می</td>\n",
       "      <td>[ZWNJ]</td>\n",
       "      <td>پردازد</td>\n",
       "      <td>.</td>\n",
       "      <td>[SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word IDs</th>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Label IDs</th>\n",
       "      <td>-100</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>O</td>\n",
       "      <td>-100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Labels</th>\n",
       "      <td>IGN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>1</td>\n",
       "      <td>IGN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0    1     2   3      4       5   6         7  8      9   \\\n",
       "Tokens     [CLS]  این  مجله  با  نگاهی  تحلیلی  به  رویدادها  و  جریان   \n",
       "Word IDs    None    0     1   2      3       4   5         6  7      8   \n",
       "Label IDs   -100    O     O   O      O       O   O         O  O      O   \n",
       "Labels       IGN    1     1   1      1       1   1         1  1      1   \n",
       "\n",
       "               10    11   12      13     14  15    16  17      18      19  20  \\\n",
       "Tokens     [ZWNJ]   های  مهم  موسیقی  ایران   و  جهان  می  [ZWNJ]  پردازد   .   \n",
       "Word IDs        8     8    9      10     11  12    13  14      14      14  15   \n",
       "Label IDs    -100  -100    O       O      O   O     O   O    -100    -100   O   \n",
       "Labels        IGN   IGN    1       1      1   1     1   1     IGN     IGN   1   \n",
       "\n",
       "              21  \n",
       "Tokens     [SEP]  \n",
       "Word IDs    None  \n",
       "Label IDs   -100  \n",
       "Labels       IGN  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words, labels = train[100][\"words\"], train[100][\"ner\"]\n",
    "tokenized_input = parsbert_tokenizer(words, is_split_into_words=True)\n",
    "word_ids = tokenized_input.word_ids()\n",
    "tokens = parsbert_tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "\n",
    "#hide_output\n",
    "previous_word_idx = None\n",
    "label_ids = []\n",
    "\n",
    "for word_idx in word_ids:\n",
    "    if word_idx is None or word_idx == previous_word_idx:\n",
    "        label_ids.append(-100)\n",
    "    elif word_idx != previous_word_idx:\n",
    "        label_ids.append(labels[word_idx])\n",
    "    previous_word_idx = word_idx\n",
    "    \n",
    "labels = [tag2index[l] if l != -100 else \"IGN\" for l in label_ids ]\n",
    "index = [\"Tokens\", \"Word IDs\", \"Label IDs\", \"Labels\"]\n",
    "\n",
    "pd.DataFrame([tokens, word_ids, label_ids, labels], index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826924b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Pytorch]",
   "language": "python",
   "name": "conda-env-Pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
